# AI Chat Assistant (Offline)

A local Streamlit-based chat assistant that uses a locally hosted Ollama model.

## Requirements

- Python 3.8+
- Ollama running locally and accessible at `http://localhost:11434`
- Recommended: create a Python virtual environment

## Install

```bash
python -m venv venv
source venv/bin/activate     # Mac/Linux
venv\Scripts\activate        # Windows
pip install -r requirements.txt
